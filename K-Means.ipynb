{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from random import randint\n",
    "\n",
    "class KMeans:\n",
    "    __valid_distance_method = ['manhattan', 'euclidian']\n",
    "    __valid_init_method = ['random', 'distribute']\n",
    "    \n",
    "    def __init__(self, n_clusters = 2, max_iter = 300, distance_method = 'euclidian', init = 'random'):\n",
    "        if (distance_method not in self.__valid_distance_method):\n",
    "            raise ValueError(\"Unknown distance calculation type type %s.\"\n",
    "                             \"Valid options are %s\" % (distance,\n",
    "                                                       self.__valid_distance_method))\n",
    "        if (n_clusters < 1):\n",
    "            raise ValueError(\"Invalid value of n_clusters, got %d. Must be at least 1\" % n_clusters)\n",
    "        self.__n_clusters = n_clusters\n",
    "        self.__max_iter = max_iter\n",
    "        self.__distance_method = distance_method\n",
    "        self.__init_method = init\n",
    "        \n",
    "    def fit(self, data):\n",
    "        if (len(data) < 2):\n",
    "            raise ValueError(\"Need at least 2 instance of data\")\n",
    "        # TODO: check type(data)\n",
    "        self.__data = data.values.tolist()\n",
    "        self.__n_data = len(data)\n",
    "        self.__attr = data.columns.tolist()\n",
    "        self.__n_attr = len(self.__attr)\n",
    "        self.labels_ = np.array([])\n",
    "        self.cluster_centers_ = []\n",
    "        self.initial_cluster_centers_idx_ = []\n",
    "        \n",
    "        if self.__init_method == 'random':\n",
    "            # randomly select k instance for initial cluster\n",
    "            for i in range(self.__n_clusters):\n",
    "                idx = randint(0, self.__n_data)\n",
    "                while (idx in self.initial_cluster_centers_idx_):\n",
    "                    idx = randint(0, self.__n_data)\n",
    "                self.initial_cluster_centers_idx_.append(idx)\n",
    "                self.cluster_centers_.append(self.__data[idx])\n",
    "        else:\n",
    "            section = int(self.__n_data / self.__n_clusters)\n",
    "            current_idx = 0\n",
    "            for i in range(self.__n_clusters):\n",
    "                self.initial_cluster_centers_idx_.append(current_idx)\n",
    "                self.cluster_centers_.append(self.__data[current_idx])\n",
    "                current_idx += section\n",
    "            \n",
    "        self.__is_convergence = False\n",
    "        self.__clusters_items = []\n",
    "        iter_count = 0\n",
    "        while (not self.__is_convergence) and iter_count < self.__max_iter:\n",
    "            self.labels_ = []\n",
    "            self.__clusters_items = [[] for _ in xrange(self.__n_clusters)]\n",
    "            \n",
    "            # iterate through all data, and calculate its distance with current centroid\n",
    "            for i in range(self.__n_data):\n",
    "                clusters_distance = []\n",
    "                for center in self.cluster_centers_:\n",
    "                    clusters_distance.append(\n",
    "                        self.__calculate_distance(self.__data[i], center, self.__distance_method)\n",
    "                    )\n",
    "\n",
    "                cluster = np.argmin(clusters_distance)\n",
    "                self.labels_.append(cluster)\n",
    "                self.__clusters_items[cluster].append(self.__data[i])\n",
    "                \n",
    "\n",
    "            old_cluster_centers = deepcopy(self.cluster_centers_)\n",
    "            # calculate new means per cluster\n",
    "            for i in range(self.__n_clusters):\n",
    "                cluster_attr_sum = np.array([0] * self.__n_attr)\n",
    "                n_member = len(self.__clusters_items[i])\n",
    "                \n",
    "                if n_member == 0:\n",
    "                    self.cluster_centers_[i] = self.__data[randint(0, self.__n_data)]\n",
    "                    continue\n",
    "                \n",
    "                for item in self.__clusters_items[i]:\n",
    "                    for j, attr in enumerate(item):\n",
    "                        cluster_attr_sum[j] += attr\n",
    "                \n",
    "                for j in range(self.__n_attr):\n",
    "                    cluster_attr_sum[j] = float(float(cluster_attr_sum[j]) / float(n_member))\n",
    "            \n",
    "            \n",
    "            # check if convergence\n",
    "            old = [item for sublist in old_cluster_centers for item in sublist]\n",
    "            new = [item for sublist in self.cluster_centers_ for item in sublist]\n",
    "            if (old[1:] == new[:-1]):\n",
    "                self.__is_convergence = True\n",
    "                    \n",
    "            iter_count += 1\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def __calculate_distance(self, X, Y, method = 'euclidian'):\n",
    "        if (len(X) != len (Y)):\n",
    "            raise ValueError(\"Can't calculate distance with across dimension\")\n",
    "        \n",
    "        if method == 'manhattan':\n",
    "            return self.__calculate_distance_manhattan(X,Y)\n",
    "        \n",
    "        return self.__calculate_distance_euclidian(X,Y)\n",
    "            \n",
    "    def __calculate_distance_euclidian(self, X, Y):\n",
    "        square_sum = 0\n",
    "        for i in range(len(X)):\n",
    "            square_sum += (X[i]-Y[i]) ** 2\n",
    "        return math.sqrt(square_sum)\n",
    "    \n",
    "    def __calculate_distance_manhattan(self, X, Y):\n",
    "        distance_sum = 0;\n",
    "        for i in range(len(X)):\n",
    "            distance_sum += abs(X[i]-Y[i])\n",
    "        return distance_sum;\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contoh pemakaian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "iris = pd.read_csv('data/Iris.csv')\n",
    "iris = iris.drop(labels='Id', axis=1)\n",
    "\n",
    "iris_data = iris.iloc[:,:-1]\n",
    "iris_label = iris.iloc[:,-1]\n",
    "species_encoder = LabelEncoder().fit(iris_label)\n",
    "iris_label_encoded = species_encoder.transform(iris_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inisiasi random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    kmeans = KMeans(n_clusters = 3, distance_method = 'euclidian', init='random').fit(iris_data)\n",
    "    mat = confusion_matrix(kmeans.labels_, iris_label_encoded)\n",
    "    purity = float(mat[0].max() + mat[1].max() + mat[2].max()) / float(mat.sum())\n",
    "    \n",
    "    print pd.crosstab(iris_label, np.array(kmeans.labels_))\n",
    "    print kmeans.cluster_centers_\n",
    "    print \"Purity: \", purity, \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(iris_label, np.array(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inisiasi distribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, distance_method = 'euclidian', init='distribute').fit(iris_data)\n",
    "mat = confusion_matrix(kmeans.labels_, iris_label_encoded)\n",
    "purity = float(mat[0].max() + mat[1].max() + mat[2].max()) / float(mat.sum())\n",
    "    \n",
    "pd.crosstab(iris_label, np.array(kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Purity: \", purity, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perbandingan dengan SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans as km\n",
    "\n",
    "xx = km(init='random', n_clusters=3).fit(iris_data)\n",
    "\n",
    "pd.crosstab(iris_label, xx.labels_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
